<!-- <!DOCTYPE html>
<!-- saved from url=(0074)https://people.engr.tamu.edu/nimak/Papers/SIGAsia2022_LookAhead/index.html -->
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Emo-Avatar: Efficient Monotonic Video Style Avatar through Deferred Neural Rendering</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/jpg" href="depth/depth_icon.jpg"> -->
  <link rel="shortcut icon" HREF="emo-avatar/favicon.ico">
  <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="./emo-avatar/bootstrap.min.css">
    <link rel="stylesheet" href="./emo-avatar/font-awesome.min.css">
    <link rel="stylesheet" href="./emo-avatar/codemirror.min.css">
    <link rel="stylesheet" href="./emo-avatar/app.css">

    <link rel="stylesheet" href="./emo-avatar/bootstrap.min(1).css">
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script type="text/javascript" async="" src="./emo-avatar/analytics.js"></script><script async="" src="./emo-avatar/js"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script>

    <script src="./emo-avatar/jquery.min.js"></script>
    <script src="./emo-avatar/bootstrap.min.js"></script>
    <script src="./emo-avatar/codemirror.min.js"></script>
    <script src="./emo-avatar/clipboard.min.js"></script>
    
    <script src="./emo-avatar/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                Emo-Avatar: Efficient Monotonic Video Style Avatar through Deferred Neural Rendering<br>
                <small>
                    Under Review
                </small>
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">

                    <li>
                        <a href="https://andypinxinliu.github.io/">
                          Pinxin Liu
                        </a>
                        <br>University of Rochester
                    </li>

                    <li>
                        <a href="https://songluchuan.github.io/">
                          Luchuan Song
                        </a>
                        <br>University of Rochester
                    </li>

                    <li>
                        <a href="https://dwan.ch/">
                          Daoan Zhang
                        </a>
                        <br>University of Rochester
                    </li>

                    <li>
                        <a href="https://hanghuacs.owlstown.net/">
                         Hang Hua
                        </a>
                        <br>University of Rochester
                    </li>

                    <li>
                        <a href="https://yunlong10.github.io/">
                         Yunlong Tang
                        </a>
                        <br>University of Rochester
                    </li>

                    <li>
                        <a href="https://www.linkedin.com/in/huaijin-tu-912627224/">
                        Huaijin Tu
                        </a>
                        <br>Georgia Institute of Technology
                    </li>

                    <li>
                        <a href="https://www.cs.rochester.edu/u/jluo/">
                         Jiebo Luo
                        </a>
                        <br>University of Rochester
                    </li>

                    <li>
                        <a href="https://www.cs.rochester.edu/~cxu22/index.html">
                         Chenliang Xu
                        </a>
                        <br>University of Rochester
                    </li>

                    
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <!-- <a href="./emo-avatar/siggraph_asia_2023_image_generation_final.pdf"> -->
                            <a href="https://arxiv.org/abs/2402.00827">
                            <img src="./emo-avatar/paper_icon.jpg" height="120px"><br>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="./emo-avatar.html">
                            <img src="./emo-avatar/paperclip.png" height="120px"><br>
                                <h4><strong>Supplementary</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="./emo-avatar.html">
                            <img src="./emo-avatar/github_pad.png" height="120px"><br>
                                <h4><strong> Code <br></strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <img src="./emo-avatar/emo-avatar_teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Artistic video portrait generation is a significant and sought-after task in the fields of computer graphics and vision. 
                    While various methods have been developed that integrate NeRFs or StyleGANs with instructional editing models for creating 
                    and editing drivable portraits, these approaches face several challenges. They often rely heavily on large datasets, 
                    require extensive customization processes, and frequently result in reduced image quality. To address the above problems, 
                    we propose the Efficient Monotonic Video Style Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's 
                    capacity for producing dynamic, drivable portrait videos. We first delved into whether pre-trained StyleGAN can encode video 
                    portraits. It has been shown that StyleGAN can encode aligned, multi-view animatable portrait videos. However, for portrait 
                    videos containing upper body elements, StyleGAN struggles to accurately reconstruct these unaligned avatars. To fully 
                    leverage the facial information from the pre-trained StyleGAN for portrait rendering. We proposed a two-stage deferred 
                    neural rendering pipeline. In the first stage, we utilize few-shot PTI inversion to initialize the StyleGAN generator 
                    through several extreme poses sampled from the video to capture the consistent representation of aligned faces from the 
                    target portrait. In the second stage, we propose a Laplacian pyramid for high-frequency texture sampling from UV maps 
                    deformed by dynamic flow of expression for motion-aware texture prior integration to provide torso features to enhance 
                    StyleGAN's ability to generate complete and upper body for portrait video rendering. Emo-Avatar reduces style 
                    customization time from hours to merely 5 minutes compared with existing methods. In addition, Emo-Avatar requires 
                    only a single reference image for editing and employs region-aware contrastive learning with semantic invariant 
                    CLIP guidance, ensuring consistent high-resolution output and identity preservation. Through both quantitative 
                    and qualitative assessments, Emo-Avatar demonstrates superior performance over existing methods in terms of 
                    training efficiency, rendering quality and editability in self- and cross-reenactment.
                    
            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Supplementary Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;">
                        <iframe align="center" width="100%" height="422"  src="https://www.youtube.com/embed/MWM0_rrnaMQ?si=z-jzpamEN6QjWRd-" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
                    </div>
            </div>
        </div> -->



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                <pre>
                @article{Zeng_2023_emo-avatar,
                    author = {Zeng, Libing and Chen, Lele and Xu, Yi and Kalantari, Nima Khademi},
                    title = {emo-avatar: A Controllable Personalized Generative Prior},
                    booktitle={ACM SIGGRAPH Asia},
                    year={2023}
                }
                </pre>
                </div>
            </div>
        </div> -->



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    We express our gratitude to the anonymous reviewers for their insightful comments and suggestions. 
                    Additionally, we would like to thank <a href="https://ykq98.github.io/">Keqiang Yan</a> and <a href="https://lyq.me/scholar">Yongqing Liang</a> for the valuable discussions.
                    The website template was borrowed from <a href="http://mgharbi.com/">Michael Gharbi</a>.
                </p>
            </div>
        </div> -->
    </div>

</body></html> -->
